%
\documentclass[12pt,letterpaper]{article}
\usepackage{bbm}
\usepackage{url}
\usepackage{fancyhdr}
%\usepackage{fancybox}
%\usepackage{amstext}
\usepackage{amsmath}
%\usepackage{rotating}
\usepackage{multicol}
\usepackage{pictexwd}
\usepackage{enumitem}
%\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{booktabs,multirow}
\usepackage{siunitx}

\setlength{\parindent}{0in}
\setlength{\textwidth}{7in}
\setlength{\evensidemargin}{-0.25in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\parskip}{.5\baselineskip}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}


%               Problem and Part
\newcounter{problemnumber}
\newcounter{partnumber}
\newcommand{\Problem}{\stepcounter{problemnumber}\setcounter{partnumber}{0}\item[\makebox{\hfil\textbf{\ \theproblemnumber.}\hfil}]}
\newcommand{\Part}{\stepcounter{partnumber}\item[(\arabic{partnumber})]}
\newcommand{\SubPart}{\stepcounter{problemnumber}\setcounter{partnumber}{0}}
\newcommand{\InPart}[1]{\stepcounter{partnumber}(\alph{partnumber})\ \ \parbox[t]{2.25in}{#1}}
\newcommand{\InSmallPart}[1]{\stepcounter{partnumber}(\alph{partnumber})\ \ \parbox[t]{1.05in}{#1}}

\pagestyle{empty}
\rhead{\large\texts{Angeline Baniqued \& Michael Wee}}
\lhead{\LARGE\textbf{CS 181: Assignment 1}}
\cfoot{}
\renewcommand{\headrulewidth}{0pt}

\begin{document}

\thispagestyle{fancy}\small

\setcounter{problemnumber}{0}
\begin{enumerate}

  \Problem \textbf{\textsc{\large{Perceptrons}}} \medskip
  
  \begin{enumerate}
    \Part 
       
        
  \end{enumerate}
  \bigskip 

  \Problem \textbf{\textsc{\large{Learning Algorithms} }} \medskip 
  \begin{enumerate} 
    \Part 
  \end{enumerate}
  \bigskip 
  
  \Problem \textbf{\textsc{\large{Neural Networks}}} \medskip
        \begin{enumerate}
        \Part See FeedForward function.\medskip
        \Part See Backprop function.\medskip
        \Part See Train function.\medskip
        \Part See EncodeLabel, GetNetworkLabel, Convert and InitializeWeights  functions.\medskip 
        \Part 
        \medskip
         
        \Part Simple Network \medskip
               \begin{enumerate}[label={\alph*) },ref={\alph*)}]
                \item \textbf{Learning rate = 1.0} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs\\
                        \begin{enumerate}[label={\roman*)},ref={\alph*)}]
                                \item
                                \item
                                \item
                        \end{enumerate} 
                        \bigskip
                \item Training set performance = \\
                      Validation set performance = \\
                      Test set performance = 
                      \bigskip                     
               \end{enumerate} \medskip
               \begin{enumerate}[label={\alph*) },ref={\alph*)}]
               \item \textbf{Learning rate = 0.1} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs\\
                        \begin{enumerate}[label={\roman*)},ref={\alph*)}]
                                \item
                                \item
                                \item
                        \end{enumerate} 
                        \bigskip
                \item Training set performance = \\
                      Validation set performance = \\
                      Test set performance = 
                      \bigskip
                \end{enumerate} \medskip
                
                \begin{enumerate}[label={\alph*) },ref={\alph*)}]
               \item \textbf{Learning rate = 0.01} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs\\
                        \begin{enumerate}[label={\roman*)},ref={\alph*)}]
                                \item
                                \item
                                \item
                        \end{enumerate} 
                        \bigskip
                \item Training set performance = \\
                      Validation set performance = \\
                      Test set performance = 
                      \bigskip
                \end{enumerate}  \medskip
                
                \begin{enumerate}[label={\alph*) },ref={\alph*)}]
               \item \textbf{Learning rate = 0.001} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs\\
                        \begin{enumerate}[label={\roman*)},ref={\alph*)}]
                                \item
                                \item
                                \item
                        \end{enumerate} 
                        \bigskip
                \item Training set performance = \\
                      Validation set performance = \\
                      Test set performance = 
                      \bigskip
                \end{enumerate} \bigskip
          \Part Hidden Network \medskip
                \begin{enumerate}[label={\alph*) },ref={\alph*)}]
                \item We used learning rates of different magnitudes such as 1.0, 0.1, 0.01 and 0.001. \medskip
                \item dfdfd
                        \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs \medskip
                \item dfd
                      \medskip
                \item dfd 
                        \medskip
                \item dfd 
                \bigskip
                \bigskip
                \end{enumerate}
          \Part Hidden Network \medskip
          \begin{enumerate}[label={\alph*)},ref={\alph*)}]
                \item We used learning rates of different magnitudes such as 1.0, 0.1, 0.01 and 0.001. \medskip
                \item dfdfd
                        \medskip
                \bigskip
                \end{enumerate}
        \end{enumerate}
     \bigskip \\
  
  \Problem \textbf{\textsc{\large{Alternative Error Function}}} \medskip
        \begin{enumerate}
        \Part The difference between the error function $C$ and the loss function $L$ is that the former penalizes having large weights $w_{km}$ and $w_{mj}$. Having large weights in this case 
        \medskip
        \Part \medskip
        $$ C(w)  = \displaystyle\sum_{n=1}^{N}\sum_{j=1}^{J}(y_{nj}-a_{nj})^2 + \lambda \sum_{m=1}^{M}\left( \sum_{k=0}^{K} w_{km}^2 + \sum_{j=1}^{J} w_{mj}^2\right) $$

        For updating weights $w_{km}$ between input $k$ and hidden input $m$, for a particular example, we have: 

        $$\displaystyle \frac{\partial{ C(w)}}{\partial w_{km}} &= \displaystyle \frac{\partial{}}{\partial w_{km}} \left(\sum_{j=1}^{J}(y_{j}-a_{j})^2\right) + \lambda \displaystyle \frac{\partial{}}{\partial w_{km }}\left( \sum_{m'=1}^{M} \sum_{k'=0}^{K} w_{k'm'}^2 \right) + \lambda \displaystyle \frac{\partial{}}{\partial w_{km }}\left( \sum_{m'=1}^{M} \sum_{j'=1}^{J} w_{m'j'}^2 \right)$$\\
The first term on the right-hand side is exactly the same as that in the loss function. We've derived the partial derivative or the first term in the Lecture 6 notes on page 9. \medskip\\ The last term or partial derivative on the right-hand side of the equation is just $0$ since $w_{km}$ doesn't appear in the expression $w_{m'j'}^2$. Thus, we only have to simplify the middle term. In the middle term, the weight $w_{km}$ only influences the $m$th hidden unit, so only one element in the summation matters. Simplifying, we get the following gradient descent:  
  \begin{equation*}
  $ \displaystyle  &= -2g'(z_m)x_k  \sum_{j=1}^{J} \delta_{j}w_{mj} + \lambda 2 w_{km}$ \medskip \\
 $ \propto \left(a_k\delta_m + \lambda w_{km}\right)   
  \end{equation} \medskip \\ 
 
 Deriving a weight update rule, we get $w_{km}^{(r+1)} \leftarrow w_{km}^{(r)} +\alpha\left(a_k\delta_m + \lambda w_{km}^{(r)}\right).$ Combining terms and constants, we have, $w_{km}^{(r+1)} \leftarrow (1 +\lambda')w_{km}^{(r)} +\alpha\left(a_k\delta_m \right) $. \bigskip

For updating weights $w_{mj}$ between hidden unit $m$ and output $j$ for a particular example, we take the partial derivatives with respect to $w_{mj}$, go through the same process and arrive at the following similar weight update rule: $w_{mj}^{(r+1)} \leftarrow (1 +\lambda')w_{mj}^{(r)} +\alpha\left(a_m\delta_j \right) $ .

        
        \end{equation}

          
        \end{enumerate}
    \bigskip \\

\end{enumerate}

        

  \end{enumerate}

  \bigskip
  
 
\end{enumerate}
\end{document}
