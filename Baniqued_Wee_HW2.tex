%
\documentclass[12pt,letterpaper]{article}
\usepackage{bbm}
\usepackage{url}
\usepackage{fancyhdr}
%\usepackage{fancybox}
%\usepackage{amstext}
\usepackage{amsmath}
%\usepackage{rotating}
\usepackage{multicol}
\usepackage{pictexwd}
\usepackage{enumitem}
%\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{booktabs,multirow}
\usepackage{siunitx}

\setlength{\parindent}{0in}
\setlength{\textwidth}{7in}
\setlength{\evensidemargin}{-0.25in}
\setlength{\oddsidemargin}{-0.25in}
\setlength{\parskip}{.5\baselineskip}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9in}


%               Problem and Part
\newcounter{problemnumber}
\newcounter{partnumber}
\newcommand{\Problem}{\stepcounter{problemnumber}\setcounter{partnumber}{0}\item[\makebox{\hfil\textbf{\ \theproblemnumber.}\hfil}]}
\newcommand{\Part}{\stepcounter{partnumber}\item[(\arabic{partnumber})]}
\newcommand{\SubPart}{\stepcounter{problemnumber}\setcounter{partnumber}{0}}
\newcommand{\InPart}[1]{\stepcounter{partnumber}(\alph{partnumber})\ \ \parbox[t]{2.25in}{#1}}
\newcommand{\InSmallPart}[1]{\stepcounter{partnumber}(\alph{partnumber})\ \ \parbox[t]{1.05in}{#1}}

\pagestyle{empty}
\rhead{\large\texts{Angeline Baniqued \& Michael Wee}}
\lhead{\LARGE\textbf{CS 181: Assignment 1}}
\cfoot{}
\renewcommand{\headrulewidth}{0pt}

\begin{document}

\thispagestyle{fancy}\small

\setcounter{problemnumber}{0}
\begin{enumerate}

  \Problem \textbf{\textsc{\large{Perceptrons}}} \medskip
  
  \begin{enumerate}
    \Part bright-or-dark (At least $75$\%\ of the pixels are on, or at least $75$\%\ of the pixels are off.)\medskip

A perceptron cannot recognize this feature. A perceptron can easily be trained to recognize one of the two conditions, for example by having all of the weights be 1 and having a threshold activation function of either 7.5 or 2.5. However, once it is set to recognize one, it cannot recognize the other because the threshold would have been set at a hard limit already. \bigskip

        \Part top-bright (A larger fraction of pixels is on in the top row than in the bottom two rows) \medskip

A perceptron can recognize this feature. We can see this by having the three weights on the top row be $\frac{1}{3}$ and all the other six weights be $-\frac{1}{6}$.  We would set the threshold to 0 and check if the sum is greater than 0 to indicate a positive example and otherwise a negative example.\bigskip

        \Part connected (The set of pixels that are on is connected. (In technical terms, this means that if we define a graph in which the vertices are the pixels that are on, and there is an edge between two pixels if they are adjacent vertically or horizontally, then there is a path between every pair of vertices in the graph.)\medskip

A perceptron cannot detect this feature. There are too many variations and combinations in terms of number of pixels that are on as well as the location of where these pixels are. For example, 1 or 2 pixels on could be a path, as can 8 or 9. We cannot do anything with the total number of pixels on. We also cannot learn weights for specific squares or specific areas of the grid because paths can appear in any location. So any way we train the weights we can come up with a counter example path that the weight set incorrectly classifies by countering the training strategy. This means either by changing the number of pixels that are on, or constructing a path on a different portion of the grid than where the weights were originally tuned to correctly classify paths. 
 
 
       
        
  \end{enumerate}
  \bigskip 

  \Problem \textbf{\textsc{\large{Learning Algorithms} }} \medskip 
  \begin{enumerate} 
    \Part The domain of handwritten digit recognition is 14x14 pixels with intensities ranging from 0 to 255. The features for this domain are the pixel intensities themselves.

\begin{itemize}
\renewcommand{\labelitemi}{$\bullet$}
\item Decision trees \medskip

        Decision trees that are modified to split on ranges of continuous values would be able to classify handwritten digits though they will neither be as effective nor as natural for this domain as other techniques such as using neural networks. The decision tree would need an effective algorithm to take into account lookahead because digit recognition revolves around combinations of pixel intensities. An algorithm analogous to ID3 would not work well because of the reliance of attributes on one another and the fact that nearby pixels affect recognition of what digit is formed. Because of the relatively large number of pixels, an effective decision tree would have to be deep because it would have to predict on combinations of features, not single attributes.  Decision trees would also have to be pruned since their tendency to overfit combined with many features (as well as the possibility to split more than once on any given continuous attribute). 
        While decision trees conceivably could be constructed to do well on digit recognition, it does not lend itself naturally to this domain. An analogous algorithm to ID3 would fail miserably. So there are decision trees that can classify the digits and we’re sure there may be algorithms that adequately do lookahead and attribute combinations, but there are more natural choices for a domain that requires combinations of features to be effective. \medskip


\item Boosted decision stumps \medskip

        Boosted decision stumps wouldn’t do well on this domain. Because boosted decision stumps split on only one attribute, they do not combine features and individually will not be able to capture patterns in the data well enough to classify digits. Individual stumps can do no better than chance since on their own they cannot predict what digit a particular pixel intensity is a part of. \medskip
\item Perceptrons \medskip

        Perceptrons would be especially bad for recognizing digits. Since there are 10 possible digits to correctly classify, it presents too much variability for a single perceptron to classify. Perceptrons have more limitations beyond just being able to output binary values, which is a problem in a 10-class domain. \medskip

\item Multi-layer feed-forward neural networks \medskip

        Multi-layer feed-forward neural networks are very suited for the domain of recognizing hand written digits. Because the digits are represented as a matrix of pixels, a neural network can propagate pixel intensities and calculate a decision boundary based on combinations of all of these features. 
        Moreover, the flexibility in designing feed-forward networks lends itself even further to classifying digits, allowing for custom hidden layers to compute and pass forward useful features like averaging/subsampling and feature maps downstream.\bigskip




  \end{enumerate}
  \bigskip 
  
  \Problem \textbf{\textsc{\large{Neural Networks}}} \medskip
        \begin{enumerate}
        \Part See FeedForward function.\medskip
        \Part See Backprop function.\medskip
        \Part See Train function.\medskip
        \Part See EncodeLabel, GetNetworkLabel, Convert and InitializeWeights  functions.\medskip 
        \Part Input data normalization is useful when we think about the shape of the sigmoid function which is the activation function we use. The sigmoid function is approximately linear when input is near 0 and converges to 1 for large input and 0 for small input. Since the activation/sigmoid function takes in the linear combination of weights and activity at each parent node, we want that linear combination to be around the vicinity of 0 which can be achieved by normalizing the input values from [0,255] to between [0,1]. \medskip

        Furthermore, if we have a wide range of input values, the bigger values will tend to
have a higher contribution to the output error, and so, the error reduction algorithm will
be focused on the higher values, neglecting the information from the small
valued variables. 
        \medskip
         
        \Part Simple Network \medskip
               \begin{enumerate}[label={\alph*)},ref={\alph*)}]
                \item \textbf{Chosen learning rate = 1.0} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs\\
                        \includegraphics[width=5in]{simple_1p0.png}
                         \begin{enumerate}[label={\roman*)},ref={\roman*)}]
                                \item Like other statistical models, neural networks are also subject to overfitting. By looking at the plot of the number of epochs of training on the x-axis, and the classification error on the y-axis, we see the standard overfitting phenomenon. \medskip
                                \item From the plot, the classification error on the validation set increases at around epoch 23, so we could train for 23 epochs and then stop there. \medskip 
                                \item It's useful to divide the dataset into three (train, validation and test) because each set serves an important purpose. We need the training set to actually train the neural network, but then during training, the classification error on the training set will continue to decrease as the number of epochs increases. Thus, we need the validation set because there will be some point after which we will be overfitting and we can detect overfitting by looking at when the validation set increases. It's important to use the validation set instead of the test set because (1) we shouldn't be peeking at the test data because the test set should be set aside for when we want to 'test' a non-overfitting neural network  and (2) without a validation set, we wouldn't be able to find a non-overfitting neural network to test. \medskip
                        \end{enumerate} 
                \item This training rate\ of 1.0 produced the highest performance on the training, validation and test sets: \\  Training set performance = 0.89\\
                      Validation set performance = 0.87\\
                      Test set performance = 0.91
                      \bigskip                     
               \end{enumerate} \medskip
               \end{enumerate}

                For reference and comparison, here are the results of the other learning rates that we didn't end up choosing\ (0.1, 0.01, 0.001). 
               \begin{enumerate}[label={\alph*) },ref={\alph*)}]
               \item \textbf{Learning rate = 0.1} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs \\
                        \includegraphics[width=5in]{simple_0p1.png}
                        \begin{enumerate}[label={\roman*)},ref={\alph*)}]
                                \item same answer as in the 1.0 learning  rate case \medskip
                                \item From the plot, the classification error on the validation set increases at around epoch 14, so we could train for 14 epochs and then stop there. \medskip 
                                \item same answer as in the 1.0 learning rate case
                        \end{enumerate} 
                        \bigskip
                \item Training set performance = 0.88\\
                      Validation set performance = 0.87\\
                      Test set performance = 0.91
                      \bigskip
                \end{enumerate} \medskip
                
                \begin{enumerate}[label={\alph*) },ref={\alph*)}]
               \item \textbf{Learning rate = 0.01} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs\\
                        \includegraphics[width=5in]{simple_0p01.png}
                        \begin{enumerate}[label={\roman*) },ref={\alph*)}]
                                \item same answer as in the 1.0 learning  rate case \medskip
                                \item From the plot, the classification error on the validation set increases at around epoch 19, so we could train for 19 epochs and then stop there. \medskip
                                \item same answer as in the 1.0 learning rate case \medskip

                        \end{enumerate} 
                        \bigskip
                \item Training set performance = 0.87 \\
                      Validation set performance = 0.86 \\
                      Test set performance = 0.91
                      \bigskip
                \end{enumerate}  \medskip
                
                \begin{enumerate}[label={\alph*) },ref={\alph*)}]
               \item \textbf{Learning rate = 0.001} \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs\\
        \includegraphics[width=5in]{simple_0p001.png}     \\
                    \begin{enumerate}[label={\roman*)},ref={\alph*)}]
                                 \item same answer as in the 1.0 learning  rate case \medskip
                                \item From the plot, since the learning rate is so small, we might require a greater number of epochs (greater than 100) to see the overfitting phenomenon more clearly. \medskip
                                \item same answer as in the 1.0 learning rate case \medskip
                        \end{enumerate} 
                        \bigskip
                \item Training set performance = 0.86 \\
                      Validation set performance = 0.85\\
                      Test set performance = 0.90
                      \bigskip
                \end{enumerate} \bigskip
          \Part Hidden Network \medskip
                \begin{enumerate}[label={\alph*) },ref={\alph*)}]
                \item We used learning rates of different magnitudes such as 1.0, 0.1, 0.01 and 0.001. \medskip
                \item dfdfd
                        \medskip
                \item Chart of Training Set and Validation Set Error vs. Number of Epochs \medskip
                \item dfd
                      \medskip
                \item dfd 
                        \medskip
                \item dfd 
                \bigskip
                \bigskip
                \end{enumerate}
          \Part Hidden Network \medskip
          \begin{enumerate}[label={\alph*)},ref={\alph*)}]
                \item We experimented with adding a second hidden layer with various numbers of hidden units. We ran experiments letting the two hidden layers have the same number of hidden units, ranging from a small number of hidden units to a larger number of hidden units. We also experimented with varying the number of hidden units in each layer, for example having 10 in the first and 15 in the second. \medskip \\
Having achieved success with one layer, we thought we could improve on this success by adding another hidden layer. We reasoned that the automatic feature engineering intuition associated with neural networks would result in the first hidden layer passing the second hidden layer an even better set of features which the second hidden layer can then do learning on.  \\
\medskip

                \item The neural networks we trained with two hidden layers universally had very poor performance on training, validation, and test sets. For all of the experiments we ran, all had training error, validation set error, and test error at 0.12 or worse. The networks would converge within three or four epochs: the errors would stop changing, indicating that the weights stopped changing. After carefully spending time carefully validating that our network was indeed constructed in the correct fashion, we determined that these poor results were not the result of a bug, but because using two hidden layers instead of one with this particular domain had very poor performance in general.\medskip \\
        The fact that the errors and weights stopped changing after a small amount of epochs indicate to us that the neural network with two hidden layers converges to a local minima extremely quickly. The fact that having two hidden layers converges rapidly to local minima that does barely better than chance indicates that one must be careful when adding additional hidden layers blindly.\medskip \\
        Because, unlike LeCun et al, we did not do feature engineering with the extra hidden layers we had, there may not have been an actual benefit to having an extra hidden layer because each hidden layer was assigned the exact same task. In fact, because each hidden layer was only given instructions to learn the general structure, they may have interfered with the function of the other, leading to vastly inferior results. Because our particular algorithm does not deal with escaping from and searching around local minima, combining that fact with the high susceptibility of two hidden layers hitting a local minima early on leads to poor results.\medskip \\
        Having two hidden layers without special engineering may also be suspect because the second hidden layer works with values that are summed and then passed through the sigmoid function which changes the distribution of values. In addition, classification now depends on the weighted output of the second layer, which in turn depends on the weighted outputs of the first layer. With all nodes feeding in to each other this could create a situation where there is a lot of interplay or interference in between layers. From this experiment we conclude that without careful feature engineering, additional hidden layers should be added only with care.

                        \medskip
                \bigskip
                \end{enumerate}
        \end{enumerate}
     \bigskip \\
  
  \Problem \textbf{\textsc{\large{Alternative Error Function}}} \medskip
        \begin{enumerate}
        \Part The difference between the error function $C$ and the loss function $L$ is that the former penalizes  large $w_{km}$ and $w_{mj}$ weights. Since we're trying to minimize $C$, we would also want to minimize the magnitude  of $w_{km}$ and $w_{mj}$. What this effectively does is prevent the network from using weights that it does not need or from overfitting the noise in the data.  
        \medskip
        \Part \medskip
        $$ C(w)  = \displaystyle\sum_{n=1}^{N}\sum_{j=1}^{J}(y_{nj}-a_{nj})^2 + \lambda \sum_{m=1}^{M}\left( \sum_{k=0}^{K} w_{km}^2 + \sum_{j=1}^{J} w_{mj}^2\right) $$

        For updating weights $w_{km}$ between input $k$ and hidden input $m$, for a particular example, we have: 

        $$\displaystyle \frac{\partial{ C(w)}}{\partial w_{km}} &= \displaystyle \frac{\partial{}}{\partial w_{km}} \left(\sum_{j=1}^{J}(y_{j}-a_{j})^2\right) + \lambda \displaystyle \frac{\partial{}}{\partial w_{km }}\left( \sum_{m'=1}^{M} \sum_{k'=0}^{K} w_{k'm'}^2 \right) + \lambda \displaystyle \frac{\partial{}}{\partial w_{km }}\left( \sum_{m'=1}^{M} \sum_{j'=1}^{J} w_{m'j'}^2 \right)$$\\
The first term on the right-hand side is exactly the same as that in the loss function. We've derived the partial derivative or the first term in the Lecture 6 notes on page 9. \medskip\\ The last term or partial derivative on the right-hand side of the equation is just $0$ since $w_{km}$ doesn't appear in the expression $w_{m'j'}^2$. Thus, we only have to simplify the middle term. In the middle term, the weight $w_{km}$ only influences the $m$th hidden unit, so only one element in the summation matters. Simplifying, we get the following gradient descent:  
  \begin{equation*}
  $ \displaystyle  &= -2g'(z_m)x_k  \sum_{j=1}^{J} \delta_{j}w_{mj} + \lambda 2 w_{km}$ \medskip \\
 $ \propto \left(a_k\delta_m + \lambda w_{km}\right)   
  \end{equation} \medskip \\ 
 
 Deriving a weight update rule, we get $w_{km}^{(r+1)} \leftarrow w_{km}^{(r)} +\alpha\left(a_k\delta_m + \lambda w_{km}^{(r)}\right).$ Combining terms and constants, we have, $w_{km}^{(r+1)} \leftarrow (1 +\lambda')w_{km}^{(r)} +\alpha\left(a_k\delta_m \right) $. \bigskip

For updating weights $w_{mj}$ between hidden unit $m$ and output $j$ for a particular example, we take the partial derivatives with respect to $w_{mj}$, go through the same process and arrive at the following similar weight update rule: $w_{mj}^{(r+1)} \leftarrow (1 +\lambda')w_{mj}^{(r)} +\alpha\left(a_m\delta_j \right) $ .

        
        \end{equation}

          
        \end{enumerate}
    \bigskip \\

\end{enumerate}

        

  \end{enumerate}

  \bigskip
  
 
\end{enumerate}
\end{document}
